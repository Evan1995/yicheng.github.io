<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BatchNorm究竟是怎么回事？</title>
    <url>/2019/12/pytorch-batchnorm-freeze/</url>
    <content><![CDATA[<p>先贴上PyTorch官网上的关于BatchNorm的公式：</p>
<script type="math/tex; mode=display">
y=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}} * \gamma+\beta</script><p>这个BatchNorm到底怎么freeze？这个函数究竟是如何成为广大网友心中的大坑的？看了好几天源码和相关的博客，我似乎有点明白了。<br>本文主要内容是<code>_BatchNorm</code>相关的源码简介。同样基于PyTorch 1.1.0。</p>
<a id="more"></a>
<h1 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">             num_features, </span></span></span><br><span class="line"><span class="function"><span class="params">             eps=<span class="number">1e-5</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">             momentum=<span class="number">0.1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">             affine=True, </span></span></span><br><span class="line"><span class="function"><span class="params">             track_running_stats=True)</span></span></span><br></pre></td></tr></table></figure>
<p>这是<code>_BatchNorm</code>的初始化的参数，无论是1d2d3d的BatchNorm都是继承自这个类，所以只需要看这个就可以了。<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNoL25uL21vZHVsZXMvYmF0Y2hub3JtLmh0bWw=" title="https://pytorch.org/docs/1.1.0/_modules/torch/nn/modules/batchnorm.html">Doc<i class="fa fa-external-link"></i></span></p>
<p>这里边很重要的两个参数就是<code>affine</code>和<code>track_running_stats</code>了。<br>跟这两个相关的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self.affine:</span><br><span class="line">    self.weight = Parameter(torch.Tensor(num_features))</span><br><span class="line">    self.bias = Parameter(torch.Tensor(num_features))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">if</span> self.track_running_stats:</span><br><span class="line">    self.register_buffer(<span class="string">'running_mean'</span>, torch.zeros(num_features))</span><br><span class="line">    self.register_buffer(<span class="string">'running_var'</span>, torch.ones(num_features))</span><br><span class="line">    self.register_buffer(<span class="string">'num_batches_tracked'</span>, torch.tensor(<span class="number">0</span>, dtype=torch.long))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>可以看到的是，<code>affine</code>是和weight和bias相关的，也就是公式中的$\gamma$和$\beta$。<br>而<code>track_running_stats</code>是和运行时的均值和方差有关的。</p>
<h1 id="affine"><a href="#affine" class="headerlink" title="affine"></a>affine</h1><p>这个参数和公式中的$\gamma$和$\beta$相关，是学习到的变量，也就是说，是通过<strong>反向传播</strong>学习到的。这里有一个小例子看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">norm = nn.BatchNorm1d(<span class="number">5</span>)</span><br><span class="line">print_norm(norm) <span class="comment"># 第一次输出</span></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">b = norm(a).sum()</span><br><span class="line">b.backward()</span><br><span class="line">print_norm(norm) <span class="comment"># 第二次输出</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(norm.parameters(), lr=<span class="number">1e5</span>)</span><br><span class="line">optimizer.step()</span><br><span class="line">print_norm(norm) <span class="comment"># 第三次输出</span></span><br></pre></td></tr></table></figure>
<p>因为weight的梯度较小，所以我把lr设置的比较大。下边是结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">norm weight Parameter containing:</span><br><span class="line">tensor([0.7289, 0.4699, 0.3586, 0.6225, 0.5753], requires_grad=True) grad None</span><br><span class="line">norm bias Parameter containing:</span><br><span class="line">tensor([0., 0., 0., 0., 0.], requires_grad=True) grad None</span><br><span class="line">********************</span><br><span class="line">norm weight Parameter containing:</span><br><span class="line">tensor([0.7289, 0.4699, 0.3586, 0.6225, 0.5753], requires_grad=True) grad tensor([ 4.7068e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.3044e-08])</span><br><span class="line">norm bias Parameter containing:</span><br><span class="line">tensor([0., 0., 0., 0., 0.], requires_grad=True) grad tensor([2., 2., 2., 2., 2.])</span><br><span class="line">********************</span><br><span class="line">norm weight Parameter containing:</span><br><span class="line">tensor([0.6819, 0.4699, 0.3586, 0.6225, 0.5846], requires_grad=True) grad tensor([ 4.7068e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.3044e-08])</span><br><span class="line">norm bias Parameter containing:</span><br><span class="line">tensor([-200000., -200000., -200000., -200000., -200000.], requires_grad=True) grad tensor([2., 2., 2., 2., 2.])</span><br></pre></td></tr></table></figure>
<p>可以看到，在step后，weight和bias被更新了上去。因为这两个参数是学习得到的，所以freeze时就显得很简单了，使用通常的方法就可以：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> para <span class="keyword">in</span> model.parameters():</span><br><span class="line">    para.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>然后在SGD的时候还得过滤一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()),</span><br><span class="line">                                    lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>
<p>这些方法百度里都可以找到。</p>
<p>在这里得到一个小结论，也就是<code>requires_grad</code>、<code>affine</code>、<code>weight</code>、<code>bias</code>这几个概念是一组相关的概念。</p>
<h1 id="track-running-stats"><a href="#track-running-stats" class="headerlink" title="track_running_stats"></a>track_running_stats</h1><p>这个参数在这里是比较<span class="label info">迷</span>的，因为这个参数是运行时的统计信息，不是反向传播学到的。<br>这个参数通常和traing是联系在一起的。通常情况下，大家说的<code>model.eval()</code>会不使用BatchNorm和Dropout，这是怎么回事？看一下源码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@weak_script_method</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">    self._check_input_dim(input)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.momentum <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        exponential_average_factor = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        exponential_average_factor = self.momentum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">and</span> self.track_running_stats:</span><br><span class="line">        <span class="keyword">if</span> self.num_batches_tracked <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.num_batches_tracked += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.momentum <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># use cumulative moving average</span></span><br><span class="line">                exponential_average_factor = <span class="number">1.0</span> / float(self.num_batches_tracked)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># use exponential moving average</span></span><br><span class="line">                exponential_average_factor = self.momentum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> F.batch_norm(</span><br><span class="line">        input, self.running_mean, self.running_var, self.weight, self.bias,</span><br><span class="line">        self.training <span class="keyword">or</span> <span class="keyword">not</span> self.track_running_stats,</span><br><span class="line">        exponential_average_factor, self.eps)</span><br></pre></td></tr></table></figure>
<p>这里是完整的forward代码，我没有做截取。截止到PyTorch 1.3.1这个函数还是这个样子。可以看到，这两个参数相关的地方就是<code>self.training and self.track_running_stats</code>和<code>self.training or not self.track_running_stats</code>两处。<br>先看第一处，它是对更新时的参数的系数做了一个计算。</p>
<p>第二处可能也是一个比较重要的，在传递到下一个函数进行处理时，它竟然直接把两个参数合并了?<strong>难怪这个函数成为广大网友心中的大坑</strong>。<br>文章[1]对这两个参数做了一些探究，写的不错，他其中引用了一个[2]中的内容，我也直接引用一下：</p>
<blockquote>
<ol>
<li><code>training=True</code>, <code>track_running_stats=True</code>, 这是常用的training时期待的行为，<code>running_mean</code> 和<code>running_var</code>会跟踪不同batch数据的mean和variance，但是仍然是用每个batch的mean和variance做normalization。</li>
<li><code>training=True</code>, <code>track_running_stats=Fals</code>e, 这时候<code>running_mean</code>和<code>running_var</code>不跟踪跨batch数据的statistics了，但仍然用每个batch的mean和variance做normalization。</li>
<li><code>training=False</code>, <code>track_running_stats=True</code>, 这是我们期待的test时候的行为，即使用training阶段估计的<code>running_mean</code>和<code>running_var</code>.</li>
<li><code>training=False</code>, <code>track_running_stats=False</code>，同2(!!!).</li>
</ol>
</blockquote>
<p>很明显的是，或操作的结果必定是3个true一个false，对应到这个例子里，也就是<strong>只有(3)传入到<code>F.batch_nrom</code>的参数才是false</strong>！另外的三个都是True啊有没有！<strong>尤其是(4)竟然变成了True??</strong><br>因为这个F里边的函数继续调用我就找不到在哪了，所以没法直接看源码了，只能分别对上述四个情况做几个小实验，测试一下。</p>
<p>为了控制结果一致，设置一个固定的weight值和a</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = torch.tensor([<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.5</span>, <span class="number">0.4</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">norm = nn.BatchNorm1d(<span class="number">4</span>, ...)</span><br><span class="line">norm.weight = Parameter(weight)</span><br><span class="line"></span><br><span class="line">a = torch.Tensor([[<span class="number">1</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="condition-1"><a href="#condition-1" class="headerlink" title="condition 1"></a>condition 1</h2><p>两个都为True时，结果似乎是最简单的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">norm = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第一次输出</span></span><br><span class="line"></span><br><span class="line">norm(a)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第二次输出</span></span><br></pre></td></tr></table></figure>
<p>结果显而易见，它学到了当前这个batch的均值和方差，这也是最基本的情况：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">norm mean tensor([0., 0., 0., 0.])</span><br><span class="line">norm var tensor([1., 1., 1., 1.])</span><br><span class="line">********************</span><br><span class="line">norm mean tensor([0.1500, 0.8500, 0.6500, 0.3500])</span><br><span class="line">norm var tensor([0.9500, 0.9500, 0.9500, 0.9500])</span><br><span class="line">********************</span><br></pre></td></tr></table></figure>
<h2 id="condition-2"><a href="#condition-2" class="headerlink" title="condition 2"></a>condition 2</h2><p>把上述代码的第一行改成<code>norm = nn.BatchNorm1d(4, track_running_stats=False)</code>时，输出的全部都是None，因为初始化的时候这部分值就已经设置成了None。即便初始化以后再执行<code>norm.track_running_stats = True</code>，结果也还是None，显然当这两个参数为None时后续也不会有什么操作会更改mean和var。</p>
<span class="label danger">但是！</span>
<p>如果程序运行起来再改成False，那么结果会怎么样？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">norm = nn.BatchNorm1d(<span class="number">4</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第一次输出</span></span><br><span class="line"></span><br><span class="line">norm.track_running_stats = <span class="literal">False</span></span><br><span class="line">norm(a)</span><br><span class="line">print_norm_mean_var(norm)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">norm mean tensor([0., 0., 0., 0.])</span><br><span class="line">norm var tensor([1., 1., 1., 1.])</span><br><span class="line">********************</span><br><span class="line">norm mean tensor([0.1500, 0.8500, 0.6500, 0.3500])</span><br><span class="line">norm var tensor([0.9500, 0.9500, 0.9500, 0.9500])</span><br><span class="line">********************</span><br></pre></td></tr></table></figure>
<p>它还是学到了！因为传入到<code>F.batch_nrom</code>的参数肯定是True，<strong>看起来，唯一能决定它学不学的，应该就是mean和var是不是None</strong>。</p>
<h2 id="condition-3"><a href="#condition-3" class="headerlink" title="condition 3"></a>condition 3</h2><p>到这个情况时，问题就复杂多了！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print_norm_mean_var(norm) <span class="comment">#第一次输出</span></span><br><span class="line"></span><br><span class="line">b = norm(a)</span><br><span class="line">print(<span class="string">'b ='</span>,b)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第二次输出</span></span><br><span class="line"></span><br><span class="line">norm.eval()</span><br><span class="line">b = norm(a)</span><br><span class="line">print(<span class="string">'b ='</span>,b)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第三次输出</span></span><br><span class="line"></span><br><span class="line">norm.train()</span><br><span class="line">b = norm(a)</span><br><span class="line">print(<span class="string">'b ='</span>,b)</span><br><span class="line">print_norm_mean_var(norm) <span class="comment">#第四次输出</span></span><br></pre></td></tr></table></figure>
<p>看一下结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">norm mean tensor([0., 0., 0., 0.])</span><br><span class="line">norm var tensor([1., 1., 1., 1.])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[-0.8000,  0.6000,  0.5000, -0.4000], [ 0.8000, -0.6000, -0.5000,  0.4000]], grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.1500, 0.8500, 0.6500, 0.3500])</span><br><span class="line">norm var tensor([0.9500, 0.9500, 0.9500, 0.9500])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[0.6977, 5.0170, 3.2575, 1.0875], [1.5184, 4.4014, 2.7445, 1.4979]], grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.1500, 0.8500, 0.6500, 0.3500])</span><br><span class="line">norm var tensor([0.9500, 0.9500, 0.9500, 0.9500])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[-0.8000,  0.6000,  0.5000, -0.4000], [ 0.8000, -0.6000, -0.5000,  0.4000]], grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.2850, 1.6150, 1.2350, 0.6650])</span><br><span class="line">norm var tensor([0.9050, 0.9050, 0.9050, 0.9050])</span><br><span class="line">********************</span><br></pre></td></tr></table></figure>
<p>第一次输出的时候是默认的。<br>第二次输出的是condition 1的情况，即学到了当前这个batch的mean和var，b也被batchnorm了。<br>第三次的结果是<code>traning=False</code>时，很明显，这一次它没有继续学习，因为这是condition 3的情况，也就是<code>self.training or not self.track_running_stats == False</code>。b被batchnorm后的结果发生了变化，是因为这一次用的是上一次学到的mean和var，而不是当前batch的，所以结果发生了变化。<br>第四次的结果是最让我意外的，因为开启了train，所以它肯定会继续学习，又回到condition 1的情况，但是，b的值似乎和第二次输出又变的一样了？</p>
<div class="note info">
            <p>在我看来，batchnrom在train模式下总是使用<strong>当前的batch的mean和var</strong>进行batchnorm，然后<strong>学习</strong>当前batch的分布，用于更新<code>running_mean</code>和<code>running_var</code>。当切换到eval模式的时候，即使用学习到的<code>running_mean</code>和<code>running_var</code>进行batchnorm。所以我们的batchsize应该大一些，且每次取到的batch尽量随机，这样才能不断地学习到整个数据集的分布。</p>
          </div>
<h2 id="condition-4"><a href="#condition-4" class="headerlink" title="condition 4"></a>condition 4</h2><p>如果初始化就把<code>track_running_stats = False</code>，<strong>那么无论是train还是eval，<code>running_mean</code>和<code>running_var</code>都是None，且每次只是用当前batch的mean和var</strong>。这里也就是[2]中所说的为什么这个同condition 2。</p>
<p>但是如果开始时true，改成False呢？<br>因为代码和condition 3完全相同，只是添加了一行<code>norm.track_running_stats = False</code>，看一下结果的神奇之处：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">norm mean tensor([0., 0., 0., 0.])</span><br><span class="line">norm var tensor([1., 1., 1., 1.])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[-0.8000,  0.6000,  0.5000, -0.4000],</span><br><span class="line">        [ 0.8000, -0.6000, -0.5000,  0.4000]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.1500, 0.8500, 0.6500, 0.3500])</span><br><span class="line">norm var tensor([0.9500, 0.9500, 0.9500, 0.9500])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[-0.8000,  0.6000,  0.5000, -0.4000],</span><br><span class="line">        [ 0.8000, -0.6000, -0.5000,  0.4000]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.2850, 1.6150, 1.2350, 0.6650])</span><br><span class="line">norm var tensor([0.9050, 0.9050, 0.9050, 0.9050])</span><br><span class="line">********************</span><br><span class="line">b = tensor([[-0.8000,  0.6000,  0.5000, -0.4000],</span><br><span class="line">        [ 0.8000, -0.6000, -0.5000,  0.4000]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br><span class="line">norm mean tensor([0.4065, 2.3035, 1.7615, 0.9485])</span><br><span class="line">norm var tensor([0.8645, 0.8645, 0.8645, 0.8645])</span><br><span class="line">********************</span><br></pre></td></tr></table></figure>
<p>首先是，它还是学习了mean和var。这和condition 2结尾得到的结论完全相同。另外，在第二次输出的时候，它依然使用的当前样本的方差，而且继续进行了学习，因为这个的第二次输出和condition 3的第三次输出<strong>完全相同</strong>。这很明显了，因为<code>self.training or not self.track_running_stats == True</code>。</p>
<h1 id="freeze"><a href="#freeze" class="headerlink" title="freeze"></a>freeze</h1><p>有了上边的介绍，相信应该已经基本清楚了，<strong>最好清楚代码处于什么位置、要什么功能</strong>，否则最好不要改<code>track_running_stats</code>这个属性。</p>
<p>在freeze的时候我看到了两种不同的说法，有一种是直接设<code>momentum = 0</code>或为<code>None</code>，这个方法看上去使得它不会学习新的分布了，但是似乎存在一点小问题，就是在fine tune的时候，他依然使用的是当前batch的mean和var，而到了测试集的时候，它却使用的是预训练数据集的mean和var，显得有一些诡异。</p>
<p>另外一个方法就是使用eval固定，这个方法可能看起来是最稳妥的，即是condition 3，直接使用从预训练的数据集上训练好的mean和var，[3]已经给出了比较详细的答案，在这里就不多做赘述。我想说的是，如果需要固定的部分没有dropout的话，似乎不需要使用apply去遍历，因为本身在执行eval的时候就会遍历这个结点的所有children，这和apply几乎是相似的。</p>
<p>当然，上述两种方法对与一个模型究竟有什么影响，甚至是否需要单独freeze batchnorm层，都应该具体问题具体分析，还是那句话，应该要清楚需要什么，才能选择适合的方法。<br>我也是初学者，如果有什么问题或者错误的地方，欢迎联系我~</p>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p>[1] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xvc2VJblZhaW4vYXJ0aWNsZS9kZXRhaWxzLzg2NDc2MDEw" title="https://blog.csdn.net/LoseInVain/article/details/86476010">Pytorch的BatchNorm层使用中容易出现的问题<i class="fa fa-external-link"></i></span><br>[2] <span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzI4MjY3MjU0Ny9hbnN3ZXIvNTI5MTU0NTY3" title="https://www.zhihu.com/question/282672547/answer/529154567">BatchNorm2d增加的参数track_running_stats如何理解? - 李韶华的回答 - 知乎<i class="fa fa-external-link"></i></span><br>[3] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9qaWFqaWEwMDcvYXJ0aWNsZS9kZXRhaWxzLzkwMTE1MTc0" title="https://blog.csdn.net/xiaojiajia007/article/details/90115174">Pytorch中的Batch Normalization layer踩坑<i class="fa fa-external-link"></i></span></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>知其所以然</tag>
        <tag>PyTorch</tag>
        <tag>BatchNorm</tag>
        <tag>freeze</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo、NexT升级与写作分支同步</title>
    <url>/2019/11/update-to-4-0/</url>
    <content><![CDATA[<p>最近一直比较忙，今天闲下来把博客升级到Hexo(<span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL25ld3MvMjAxOS8xMC8xNC9oZXhvLTQtcmVsZWFzZWQv" title="https://hexo.io/news/2019/10/14/hexo-4-released/">4.0.0<i class="fa fa-external-link"></i></span>)和NexT(7.5.0)，然后把NexT的配置文件移动到Hexo的配置文件中，这样以后NexT的升级就不用总是改配置文件了。<br>顺便把Hexo的写作分支push到Github，这个放了好久了，今天也要做一下。</p>
<a id="more"></a>
<h1 id="Hexo升级"><a href="#Hexo升级" class="headerlink" title="Hexo升级"></a>Hexo升级</h1><p>（感觉这个升级比较有风险，因为我对hexo怎么组织的确实不太了解…）<br>我首先把hexo-cli升级了，在这里直接用<code>npm outdated -g</code>看一下是否需要升级，然后<code>npm install hexo-cli@latest -g</code>。可以不加<code>@latest</code>，那就会升级到<code>Wanted</code>的版本，这样会稳一些，我直接到了最新版本。<br>然后再到hexo的文件中，<code>npm outdated</code>看一下要升级的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">λ npm outdated</span><br><span class="line">Package                  Current  Wanted  Latest  Location</span><br><span class="line">eslint                     6.0.1   6.7.0   6.7.0  hexo-site</span><br><span class="line">hexo                       3.9.0   3.9.0   4.0.0  hexo-site</span><br><span class="line">hexo-deployer-git          1.0.0   1.0.0   2.1.0  hexo-site</span><br><span class="line">hexo-generator-archive     0.1.5   0.1.5   1.0.0  hexo-site</span><br><span class="line">hexo-generator-category    0.1.3   0.1.3   1.0.0  hexo-site</span><br><span class="line">hexo-generator-index       0.2.1   0.2.1   1.0.0  hexo-site</span><br><span class="line">hexo-generator-searchdb    1.0.8   1.2.0   1.2.0  hexo-site</span><br><span class="line">hexo-generator-tag         0.2.0   0.2.0   1.0.0  hexo-site</span><br><span class="line">hexo-renderer-ejs          0.3.1   0.3.1   1.0.0  hexo-site</span><br><span class="line">hexo-renderer-stylus       0.3.3   0.3.3   1.1.0  hexo-site</span><br><span class="line">hexo-server                0.3.3   0.3.3   1.0.0  hexo-site</span><br></pre></td></tr></table></figure><br>然后用<code>npm install hexo@latest -save</code>一个一个的升级了。</p>
<p>这一步走下来，效果怎么样没看出来，但是，没翻车，谢天谢地。</p>
<p>最重要的是更新配置文件！参考github上最新的配置文件<code>_config.yml</code><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hleG9qcy9oZXhvLXN0YXJ0ZXIvYmxvYi9tYXN0ZXIvX2NvbmZpZy55bWw=" title="https://github.com/hexojs/hexo-starter/blob/master/_config.yml">link<i class="fa fa-external-link"></i></span>，注意参考官方<span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvY29uZmlndXJhdGlvbg==" title="https://hexo.io/docs/configuration">Docs<i class="fa fa-external-link"></i></span>。</p>
<h1 id="NexT更改配置位置"><a href="#NexT更改配置位置" class="headerlink" title="NexT更改配置位置"></a>NexT更改配置位置</h1><p>之前我的版本是NexT 7.2.0，现在的最新版是NexT 7.5.0，我先从Github下载一个最新版的，<strong>存在一个其他的文件夹</strong>，然后把它恢复到7.2.0（直接下7.2.0也行）。<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy9kb2NzL2dldHRpbmctc3RhcnRlZC9pbnN0YWxsYXRpb24=" title="https://theme-next.org/docs/getting-started/installation">方法<i class="fa fa-external-link"></i></span><br>把<code>_config.yml</code>复制出来，用vscode和我的作比较，看一下我都更改了什么地方。然后把我更改的配置全部移动到hexo的<code>_config.yml</code>下，参考<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy9kb2NzL2dldHRpbmctc3RhcnRlZC9kYXRhLWZpbGVz" title="https://theme-next.org/docs/getting-started/data-files">这里<i class="fa fa-external-link"></i></span>的Hexo-Way和<span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvY29uZmlndXJhdGlvbiNPdmVycmlkaW5nLVRoZW1lLUNvbmZpZw==" title="https://hexo.io/docs/configuration#Overriding-Theme-Config">这里<i class="fa fa-external-link"></i></span>。</p>
<div class="note info">
            <p>需要注意的是，如果有个子属性要修改，一定要把父属性全部拷贝过来！例如footer整个的属性全部复制过来，然后再更改，如果仅复制一部分，那么没有复制的就会出不来。</p>
          </div>
<h1 id="NexT升级"><a href="#NexT升级" class="headerlink" title="NexT升级"></a>NexT升级</h1><p>如果仅仅是把NexT的配置移动到hexo的配置中，上边这一步足矣。如果需要升级，则继续。<br>上一步结束以后，再把新下载的NexT恢复到7.5，然后再和自己的配置文件比一下，看一下都更新了那些地方，刚才的配置是不是过时了，因为有一些配置的格式发生了变化。从7.2到7.5改动还是比较大的。以后每次更新的时候，只需要查看当前版本和之前版本的区别，然后修改hexo的配置文件就行了。</p>
<p>如果对NexT的模板进行了魔改，那么还需要把魔改的部分每次更新都要改，我之前改过，后来更新了就放弃了，默认的这些东西都挺好用的。</p>
<div class="note danger">
            <h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>第一个需要注意的是NexT 7.4.0的时候<code>fi</code>这个Tag被取消了(7.4.0更新<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy9uZXh0LTctNC0wLXJlbGVhc2VkLw==" title="https://theme-next.org/next-7-4-0-released/">link<i class="fa fa-external-link"></i></span>)，所以如果之前的图片用过<code>fi</code>的，记得改回Hexo的格式，不然会编译不出来。<br>第二个是，升级上来后使用related_posts的话记得安装，不然会打不开网页。<code>npm install hexo-related-popular-posts --save</code></p>
          </div>
<h1 id="同步写作分支"><a href="#同步写作分支" class="headerlink" title="同步写作分支"></a>同步写作分支</h1><p>在执行<code>hexo init</code>的时候，就会自动调用一串的脚本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">INFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.git</span><br><span class="line">Cloning into <span class="string">'G:\Blogtest\xxxx'</span>...</span><br><span class="line">...</span><br><span class="line">Submodule <span class="string">'themes/landscape'</span> (https://github.com/hexojs/hexo-theme-landscape.git) registered <span class="keyword">for</span> path <span class="string">'themes/landscape'</span></span><br><span class="line">Cloning into <span class="string">'G:/Blogtest/test/themes/landscape'</span>...</span><br><span class="line">...</span><br><span class="line">Submodule path <span class="string">'themes/landscape'</span>: checked out <span class="string">'73a23c51f8487cfcd7c6deec96ccc7543960d350'</span></span><br><span class="line">INFO  Install dependencies</span><br><span class="line">...</span><br><span class="line">added 362 packages from 469 contributors <span class="keyword">in</span> 25.55s</span><br><span class="line"></span><br><span class="line">INFO  Start blogging with Hexo!</span><br></pre></td></tr></table></figure>
<p>先是：把项目<code>hexo-starter</code>拉过来；<br>这个项目里有一个submodule，就是模板landscape，即默认的模板，随后就会安装这个submodule。<br>最后安装依赖。</p>
<p>（操作之前最好先压缩一个压缩包，如果有问题可以恢复回来）<br>我重新使用<code>git init</code>给当前文件夹创建版本库。然后给<code>.gitignore</code>添加了一行<code>themes/</code>，也就是我没有同步任何模板的信息，因为我的模板没有魔改，需要下载的时候只需要重新从GitHub下载即可。如果需要同步模板，可能需要删除掉模板的<code>.git</code>文件。<br>我在Github创建了一个新的private的仓库，然后把这个项目同步了上去。</p>
<p>我又在另外一个文件夹clone一下，看一下效果。</p>
<ul>
<li><code>git clone git@github.com:yourid/Hexo-Blog-Backup.git</code>把项目拉下来</li>
<li><code>cd Hexo-Blog-Backup\</code>然后<code>npm install</code>。只要之前所有的npm在安装时都用上了<code>-save</code>，那么所有的依赖都会写在<code>package.json</code>中，使用这个指令就会自动安装</li>
<li><code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code>把模板拿回来<br>可以看到，这和执行<code>hexo init</code>经过了同样的过程。安装好后的目录结构和原来也是完全一样。然后运行一下看看效果，没有发现BUG~</li>
</ul>
<p>最后一个问题是，这样同步了以后，会丢失<code>.deploy_git</code>文件夹，这里边存着之前提交到github的记录<code>.git</code>文件，我的建议是可以完成这几步以后，把部署分支clone下来，改名叫做<code>.deploy_git</code>放到刚才新生成的文件夹中。(我还没试过…)</p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo 3.9</tag>
        <tag>NexT 7</tag>
        <tag>Hexo 4.0</tag>
        <tag>NexT 7.5</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch backward 参数解读</title>
    <url>/2019/09/pytorch-backward-parameter/</url>
    <content><![CDATA[<p>官网Doc链接：<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL3RlbnNvcnMuaHRtbCN0b3JjaC5UZW5zb3IuYmFja3dhcmQ=" title="https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.backward">link<i class="fa fa-external-link"></i></span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">backward(gradient=<span class="literal">None</span>, retain_graph=<span class="literal">None</span>, create_graph=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>看一下第一个参数的意思是什么。其实不管Tensor中是几个变量，都可以传入参数，但是一般情况下，我们的out都是只有一个标量的tensor，可以不传入参数。</p>
<p>举个例子，通常我们在求导的时候都是对一个变量求导，例如</p>
<script type="math/tex; mode=display">
F=H+G</script><p>所以当我们对<script type="math/tex">F</script>求导的时候很自然的可以使用链式法则</p>
<script type="math/tex; mode=display">
\frac{d F}{d x}=\frac{d F}{d H}\frac{d H}{d x}+\frac{d F}{d G}\frac{d G}{d x}</script><a id="more"></a>
<p>但是当我们开始求导的位置是<script type="math/tex">H</script>和<script type="math/tex">G</script>时，如<code>torch.Tensor([H,G])</code>，程序就并不知道这个要从那里开始了，这两个参数有什么关系，所以我们就得给这两个变量<em>假想</em>一个关系，假如存在一个关系</p>
<script type="math/tex; mode=display">K=\sigma(H, G)</script><p>这个关系是什么不重要，重要的是我们在backward中传入的参数就是一个二元组，分别是</p>
<script type="math/tex; mode=display">\frac{d K}{d H}, \frac{d K}{d G}</script><p>这两个参数分别是</p>
<script type="math/tex; mode=display">\frac{d H}{d x}, \frac{d G}{d x}</script><p>的系数。程序最后就可以把这两个关系用上，继续求导。</p>
<p>所以在官网的教程中写道：</p>
<blockquote>
<p>out.backward() is equivalent to out.backward(torch.tensor(1.))</p>
</blockquote>
<p>也就不难理解了。也就是说可能存在一个关系</p>
<script type="math/tex; mode=display">L=\mu(F)</script><p>在开始求导的时候，这个系数的值</p>
<script type="math/tex; mode=display">\frac{d L}{d F} = 1</script>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>知其所以然</tag>
        <tag>PyTorch</tag>
        <tag>backward</tag>
      </tags>
  </entry>
  <entry>
    <title>如何设置PyTorch的动态学习率</title>
    <url>/2019/08/how-to-set-lr-in-pytorch/</url>
    <content><![CDATA[<p>本文主要涉及内容：<code>Optimizer</code>、<code>_LRScheduler</code>等源码分析。<br>本文依旧基于PyTorch 1.1.0。</p>
<a id="more"></a>
<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p>PyTorch提供了<code>torch.optim.lr_scheduler</code>来帮助用户改变学习率，下边将从<code>Optimizer</code>入手，看一下这个类是如何工作的。</p>
<p>为什么从Optimizer入手，因为无论是Adam还是SGD，都是继承的这个类。同时，scheduler也是给所有的Optimizer服务的，所以需要用的方法都会定义在这个基类里，直接看一下这个类的属性即可。给出Doc中的代码<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNoL29wdGltL29wdGltaXplci5odG1sI09wdGltaXplcg==" title="https://pytorch.org/docs/1.1.0/_modules/torch/optim/optimizer.html#Optimizer">链接<i class="fa fa-external-link"></i></span>。</p>
<p>首先是初始化方法<code>def __init__(self, params, defaults)</code>，这个方法的params参数，就是我们在初始化优化器的时候传入的网络的参数，如<code>Alexnet.parameters()</code>，而后边所有的参数都将合并成dict参数作为这个方法的defaults。<br>看一下<code>Alexnet.parameters()</code>中存的都是什么：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> alex <span class="keyword">in</span> Alexnet.parameters():</span><br><span class="line">    print(alex.shape)</span><br></pre></td></tr></table></figure><br>可以看到，这里边存的就是整个网络的参数。<br>有两种定义optimizer的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: model.base.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: model.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>如果是<strong>第一种</strong>定义的方法：在这个初始化方法中，会把这些参数先改造成<code>[{&#39;params&#39;: Alexnet.parameters()}]</code>这样的一个长度为1的list。然后对这个list进行加工，添加上defaults中的参数，如果我们使用Alexnet来做一个例子的话，就是下边这个样子：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(Alexnet.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">print([group.keys() <span class="keyword">for</span> group <span class="keyword">in</span> optimizer.param_groups])</span><br><span class="line"><span class="comment"># [dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])]</span></span><br></pre></td></tr></table></figure><br>如果是<strong>第二种</strong>定义的方法：因为传入的本身就是dict的形式，所以会继续对他进行加工，添加上后边的参数，我们直接看疗效：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: Alexnet.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: Alexnet.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">print([group.keys() <span class="keyword">for</span> group <span class="keyword">in</span> optimizer.param_groups])</span><br><span class="line"><span class="comment"># [dict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov']), dict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov'])]</span></span><br></pre></td></tr></table></figure><br>这次的list变成了两个元素，而且每个元素的组成和使用Adam也不一样了，这很明显，因为不同的优化器需要的参数不同嘛~(关于不同层的lr不同的设置这里给出官网<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL29wdGltLmh0bWwjcGVyLXBhcmFtZXRlci1vcHRpb25z" title="https://pytorch.org/docs/1.1.0/optim.html#per-parameter-options">链接<i class="fa fa-external-link"></i></span>)</p>
<p>但是两者是相似的，就是每个元素都有params和lr，这就够了。</p>
<h1 id="LRScheduler"><a href="#LRScheduler" class="headerlink" title="_LRScheduler"></a>_LRScheduler</h1><p>所有的动态修改lr的类，都是继承的这个类，所以我们看一下这个类包含什么方法。源码<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNoL29wdGltL2xyX3NjaGVkdWxlci5odG1sI0V4cG9uZW50aWFsTFI=" title="https://pytorch.org/docs/1.1.0/_modules/torch/optim/lr_scheduler.html#ExponentialLR">链接<i class="fa fa-external-link"></i></span>。</p>
<p>在初始化方法中<code>def __init__(self, optimizer, last_epoch=-1)</code>，包含两个参数，第一个参数就是我们上边提到的optimizer的任何一个子类。第二个参数的意思是<strong>当前执行到了哪个epoch</strong>。我们不指定它的时候，虽然默认是-1，但是init中会调用一次step并设置为0。</p>
<div class="note danger">
            <p>一定要注意PyTorch的版本！我的windows上用的是1.0.1，服务器用的是1.1.0，就闹了很多问题。就拿这个类来说，在1.0.1中是先<code>setp()</code>再训练，而1.1.0进行了更新，先训练，然后再<code>step()</code>。</p>
          </div>
<p>当我们调用了初始化后，会给optimizer增加一个字段，看一下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">5</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">print([group.keys() <span class="keyword">for</span> group <span class="keyword">in</span> optimizer.param_groups])</span><br><span class="line"><span class="comment"># [dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'initial_lr'])]</span></span><br></pre></td></tr></table></figure><br>新增加的<code>initial_lr</code>字段就是原始的lr。</p>
<p>在<code>def step(self, epoch=None)</code>方法中，通常情况下我们<strong>不需要</strong>指定这个参数epoch，因为每次调用他都会增加1。在这个函数中会调用一个需要重载的方法<code>get_lr()</code>，每次调用都会从这个方法中提取改变后的lr，赋值给optimizer。</p>
<div class="note info">
            <p>这里其实我一直有个疑问的，就是scheduler的step和optimizer的step是一个什么关系，其实通过源码，看到这里，这俩函数没啥关系！scheduler的step只会修改lr，两者都需要执行！</p>
          </div>
<p>下边看一下两个scheduler的<code>get_lr()</code>对比一下。先看一下SetpLR：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (self.last_epoch == <span class="number">0</span>) <span class="keyword">or</span> (self.last_epoch % self.step_size != <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">return</span> [group[<span class="string">'lr'</span>] <span class="keyword">for</span> group <span class="keyword">in</span> self.optimizer.param_groups]</span><br><span class="line">    <span class="keyword">return</span> [group[<span class="string">'lr'</span>] * self.gamma</span><br><span class="line">            <span class="keyword">for</span> group <span class="keyword">in</span> self.optimizer.param_groups]</span><br></pre></td></tr></table></figure><br>这个会在设置的步长的整倍数的时候将lr*gamma。<br>而ExponentialLR则会在每轮结束的时候都进行乘gamma的操作，这个减小也真的是指数倍的。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.last_epoch == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> self.base_lrs</span><br><span class="line">    <span class="keyword">return</span> [group[<span class="string">'lr'</span>] * self.gamma</span><br><span class="line">            <span class="keyword">for</span> group <span class="keyword">in</span> self.optimizer.param_groups]</span><br></pre></td></tr></table></figure></p>
<h1 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">train_loader = Data.DataLoader(</span><br><span class="line">        dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_loader:</span><br><span class="line">        ...</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>知其所以然</tag>
        <tag>PyTorch</tag>
        <tag>learning rate</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch读取数据入门</title>
    <url>/2019/08/pytorch-with-big-dataset/</url>
    <content><![CDATA[<p>本文主要涉及内容：<code>Dataset</code>、<code>DataLoader</code>、<code>DatasetFloder</code>等相关源码分析。<br>看了网上众多的关于这方面的文章，详细解释道理的很少，看完之后我依然很迷惑。所以我在看过看过网上一些教程和PyTorch的部分源码以后写了一些总结，基本上也是我自己在学习这部分内容时的一个经过。文章前边可能有些地方不可避免地走了弯路，但是我也希望如果你是初学者，建议看一看，思考一下，和最后的方法作对比。<br>本文基于PyTorch 1.1.0。</p>
<a id="more"></a>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>忽略基础知识直接进入<a href="#看源码">看源码</a>。</p>
<p>可能需要导入的包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>
<h2 id="Dataset基础类"><a href="#Dataset基础类" class="headerlink" title="Dataset基础类"></a>Dataset基础类</h2><p>首先是Dataset基础类，所有的要传入DataLoder的类都要继承这个类才行，同时必须重载<code>__getitem__</code>和<code>__len__</code>这两个方法。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure><br>可以看到，Dataset提供了一个<code>__add__</code>方法，用于两个Dataset的<strong>相加</strong>，返回一个ConcatDataset对象。初始化时简单的以list的形式，把传入的参数合并在一起。</p>
<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>定义Transforms可以对读入的数据进行一些变换操作，可以放下如下位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ..., transforms=None)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        data = ...</span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = self.transforms(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<p>这里的<code>self.transforms</code>有两层含义，一方面是可以传入一个transform，另外一方面，也可以在初始化方法中自己定义一组transform，使用链式定义的也可以，使用<code>transforms.Compose()</code>定义也可以。</p>
<h2 id="结合Pandas读取图片"><a href="#结合Pandas读取图片" class="headerlink" title="结合Pandas读取图片"></a>结合Pandas读取图片</h2><p>可以在初始化的过程中读入csv文件，csv文件中存好相关的配置项，然后在getitem的时候在依照配置项读取图片。这样的好处是，配合DataLoader可以边读边训练，不用先花费大量时间把图片读进来在训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_path)</span>:</span></span><br><span class="line">        <span class="comment"># 读取 csv 文件</span></span><br><span class="line">        self.data_info = pd.read_csv(csv_path, header=<span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 下边可以对各个列进行解析</span></span><br><span class="line">        ...</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># 读取图像</span></span><br><span class="line">        img_as_img = Image.open(self.image[index])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 在csv中可以配置，是否需要额外操作</span></span><br><span class="line">        <span class="keyword">if</span> self.operation_arr[index]:</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 可以把图像转换成tensor，然后从csv中读取出label信息，一起返回</span></span><br><span class="line">        <span class="keyword">return</span> (img_as_tensor, image_label)</span><br></pre></td></tr></table></figure>
<p><a href="#reference">[1]</a>中还给出了怎么从csv中读取像素值，然后转换成图片在返回，和上述过程大同小异。<br><div class="note warning">
            <p>但是，存在的问题是，DataLoader的shuffle还能不能用了，这个问题对训练来说非常重要。答案是可以！详情请继续读。</p>
          </div></p>
<h2 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h2><p>Dataset使得<strong>An image is read from the file on the fly</strong>，但是缺少了三个功能：batching、shuffling、multiprocessing。<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvZGF0YV9sb2FkaW5nX3R1dG9yaWFsLmh0bWwjaXRlcmF0aW5nLXRocm91Z2gtdGhlLWRhdGFzZXQ=" title="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#iterating-through-the-dataset">link<i class="fa fa-external-link"></i></span>Dataloader则提供了这些功能。<br>配合Dataloader读取数据，for循环外可加一层循环控制epoch。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    custom_dataset = MyDataset(...)</span><br><span class="line">    train_loader = Data.DataLoader(dataset=custom_dataset,</span><br><span class="line">                                    batch_size=BATCH_SIZE,</span><br><span class="line">                                    shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> step, (image, label) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h1 id="借鉴"><a href="#借鉴" class="headerlink" title="借鉴"></a>借鉴</h1><p>这里分析两个前辈给出的解决方案，看一下他们的方案有什么优缺点。</p>
<p><a href="#reference">[3]</a>中使用的方法，每次读取一个数据，基本可看作<strong>模仿</strong><a href="#结合Pandas读取图片">上边</a>过程的应用，问题是无法shuffle。</p>
<p><a href="#reference">[2]</a>中提及了一个比较完美的局部shuffle的解法，具体代码详见链接，这里对他的思想做个说明。<br>首先，他就提到了任务和要求。<span class="label primary">任务就是数据量大，无法一次读到内存中。</span>其次是要求，首先是每次读取一部分数据，然后是重点，能够<span class="label primary">shuffle</span>。能否shuffle对于训练很重要。在这个任务中，把数据存在了csv中，但是从<a href="#结合Pandas读取图片">上边</a>看，其实存储在csv中图片的路径等形式依然可以采取这种方式。</p>
<ul>
<li><code>__init__</code>方法，<code>__init__(self,file_path,nraws,shuffle=False)</code>，这个方法的nraws参数是用来定义每次读取多少行进行shuffle，此外，还在这个方法中计算了这个csv一共多少行。这个写法同样适用于读取图片地址。</li>
<li><code>def initial(self)</code>在这个方法中，是现实读入nraws行，然后对着nraws进行shuffle，这个nraws应该对于batch来说稍微大一些，不然这个局部shuffle的意义就没了。比如作者在这里就使用的<code>nraws=1000</code>和<code>batch_size=64</code>。为什么不写入init中，因为这个方法每个epoch之前都要用。</li>
<li><code>__len__</code>不用多说，直接读取计算出来的行数。</li>
<li><code>__getitem__</code>的重载即是每次都队列的前端拿取一个元素，如果队列为空了，则重新读取一个nraws个元素。这个方法和initial共同维持了一个队列，数据结构的美妙之处。</li>
</ul>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>读取图片的速度和totensor的转换速度共同影响了dataloader的时间，而且totensor占据了主要部分，可以在预处理的时候把所有的数据以<code>pkl</code>(或<code>bin</code>)的形式存储好，这样读取的时候直接读取的就是tensor的数据，这样可以显著加快速度。</p>
<p>对于<a href="#reference">[2]</a>提出的方法，可以直接把一块数据存储成一个pkl，如1000个图片存成一个pkl，不用重新totensor，这样的整体速度会相对快，可以对这一组数据进行shuffle。这种改变不影响代码的整体结构，整体思路不变。</p>
<h2 id="再改进"><a href="#再改进" class="headerlink" title="再改进"></a>再改进</h2><p><strong>一代目</strong>步骤，基于上述两个方法的一个小方法(<em>下文还会对这个步骤继续改进</em>)：</p>
<ol>
<li>预处理的图片totensor后全部转化成pkl，不过这次是一对一的转换，这个转化可能需要很多时间，但是这却会给以后的训练节约很多时间。</li>
<li>把转化后的pkl地址和label对应着存到csv中，我的路径方式是<code>./lable/xxx.pkl</code>，在解析的时候只需要把label和地址拼接起来即可组成正确的相对路径。此外，在这里还可以配置是否需要其他操作。<strong>用csv的形式变化更加多样，可以实现更复杂的功能。</strong></li>
<li>定义一个函数，这个函数的功能是，读取这个csv文件，然后得到lenth，对这个lenth进行shuffle。</li>
<li>得到一个shuffle之后的序列，使用这个序列传入MyDataset，按照shuffle之后的顺序读取，再配合DataLoader即可每次都拿到shuffle之后的数据。等到这个epoch结束后，再shuffle，即可进行下次训练。</li>
</ol>
<p>还可以先进行数据的划分，如使用scikit-learn的分层划分，然后再对train set进行shuffle。这样需要对shuffle进行改造，即不对validation set进行shuffle，这样可以实现了训练集与测试集的划分。</p>
<h1 id="看源码"><a href="#看源码" class="headerlink" title="看源码"></a>看源码</h1><p>通过看源码来获取一些新的视角，来帮助思考，DataLoader<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNoL3V0aWxzL2RhdGEvZGF0YWxvYWRlci5odG1sI0RhdGFMb2FkZXI=" title="https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html#DataLoader">源码<i class="fa fa-external-link"></i></span>以及它调用的Sampler的<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNoL3V0aWxzL2RhdGEvc2FtcGxlci5odG1s" title="https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/sampler.html">源码<i class="fa fa-external-link"></i></span>。</p>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>关于DataLoader的定义，直击要害，直接看两个最关键的点。第一段代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> batch_sampler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> sampler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            sampler = RandomSampler(dataset)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sampler = SequentialSampler(dataset)</span><br><span class="line">    batch_sampler = BatchSampler(sampler, batch_size, drop_last)</span><br></pre></td></tr></table></figure><br>这段代码解释了为啥dataset需要重载<code>__len__</code>，实际上是传递给了各种Sampler做下一步的处理。在DataLoader的定义中<span class="label info">没有使用dataset的长度信息</span>。</p>
<p>然后关注DataLoader中的第二段代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> _DataLoaderIter(self)</span><br></pre></td></tr></table></figure><br>这个可以使得DataLoader进行迭代，即可以使用for循环。廖雪峰老师的教程里有写到，这个方法如果返回<code>xxx</code>，则会不断的调用<code>xxx.__next__</code>，廖雪峰老师说通常这个就会返回<code>self</code>，返回对自己的迭代，这里则直接祭出了高级用法，返回一个对象了，使用这个对象的<code>__next__</code>。</p>
<h2 id="DataLoaderIter"><a href="#DataLoaderIter" class="headerlink" title="_DataLoaderIter"></a>_DataLoaderIter</h2><p>可以看到，这个方法实际是我们在使用for循环的时候扮演者重要角色的大BOSS。这个方法的注释中写了大量的内容，包括数据流、多线程遇到的问题，这部分不多赘述。先看一下单线程的方法，即<code>num_workers=0</code>的情况(TODO:补充多线程)：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:  <span class="comment"># same-process loading</span></span><br><span class="line">        <span class="comment"># 前提 self.sample_iter = iter(self.batch_sampler)</span></span><br><span class="line">        indices = next(self.sample_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br><span class="line">        <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><br>可以看到，它又通过<code>batch_sampler</code>的迭代获取序号的列表生成器，然后根据<strong>序号集合indices</strong>进行读取数据，再组合成一个batch。<code>StopIteration</code>这个用法真是绝了，廖雪峰老师也提到过，用for循环调用generator时，是拿到不到返回的值的，所以当报错的时候即为for循环结束的地方，也就是完成了一次epoch的训练。<br><code>collate_fn</code>是一个callable的函数，如果不传入自定义的函数，则调用默认的。如果你的合并操作需要一些特殊的操作，可以自己定义这个函数，那么可以参考官方文档，那里给出了一个例子。如果使用默认的，则直接贴出源码<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3B5dG9yY2gvcHl0b3JjaC9ibG9iL21hc3Rlci90b3JjaC91dGlscy9kYXRhL191dGlscy9jb2xsYXRlLnB5I0w0Mw==" title="https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py#L43">链接<i class="fa fa-external-link"></i></span>。可以看到这个默认的函数非常之强大，精通各种合并。总之，合并后的数据集是一个batch。<br>这里是唯一一次用到<code>__getitem__</code>方法。下文会有分析。</p>
<h2 id="BatchSampler"><a href="#BatchSampler" class="headerlink" title="BatchSampler"></a>BatchSampler</h2><p>BatchSampler是对其他的Sampler进行了封装，代码非常简单。<br>有意思的的关于Batchs长度的计算，用全部数据除以batch size(单纯的觉得这里很有意思)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> self.sampler:</span><br><span class="line">    batch.append(idx)</span><br><span class="line">    <span class="keyword">if</span> len(batch) == self.batch_size:</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">        batch = []</span><br></pre></td></tr></table></figure>
<p>这段代码表示，每次从sampler中获取的都是一个<strong>序号</strong>，把这个序号组合到batch size的大小，然后yield。</p>
<p>关于使用到的这两个Sampler。SquentialSampler就不多说了，顺序直接迭代，返回一二三四五六七。RandomSampler则是对序号打乱顺序，这里多说一点，按照这个程序中的调用，返回的是<code>torch.randperm(n)</code>，即n个数，没有重复的。</p>
<p>这里的n，就是<code>n = len(self.data_source)</code>，<strong>也就是我们一开始重载的<code>__len__</code></strong>，直到这里才起作用。</p>
<p>此外，这个Random还支持一个叫做replacement的参数，即可以有重复的取样。这个函数还有更厉害的组合拳，可以去看源码。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><div class="note success">
            <h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>至此，大概的脉络已经分析出来了。如何调用大数据的时候写好我们的Dataset类呢？我们可以看到两个重载的方法出现的位置，答案似乎已经清楚了，就是<a href="#结合Pandas读取图片">结合Pandas读取图片</a>一节描述的内容。只要我们能够清楚地知道csv文件的长度，那么Dataloader就会帮我们进行shuffle，而不用我们自己进行任何操作。在读取图片的时候，也就是上文唯一提到的<code>__getitem__</code>出现的位置，我们只需要按照index的位置获取csv中的地址，读入图片即可，如果已经转换成了pkl，那么会大大加快我们的速度。</p>
          </div>
<p><strong>二代目</strong>步骤：<br>维持一代目的1、2步骤不变，3、4的步骤可以交给DataLoader来完成了~也就是说我们只需要定义好myDataSet中的<code>__getitem__</code>，用好index这个关键变量，那么剩下的工作就交给DataLoader的shuffle就行了。</p>
<p>那么现在再来看这三个引用。ref1悄悄地告诉了我们最终结果，但是它却没有解释清楚为什么要这么用。ref2提出了一个局部shuffle的办法，也是一个不错的点子。<strong>同时也需要注意的是，这个方法中把<code>__getitem__</code>当作了<code>__iter__</code>来用，因为每次总是取队列的最顶端，没有使用到传入的index参数。</strong> ref3最主要的问题也是没有用到index。所以ref2和ref3无论DataLoader的shuffle参数是True还是False，都无关紧要了，因为根本不会用到这个属性。</p>
<h1 id="简化策略"><a href="#简化策略" class="headerlink" title="简化策略"></a>简化策略</h1><p>其实，上边使用csv的形式，会提供更大的<strong>可操作性</strong>，可以自己定制更加灵活的形式解决自己的问题。</p>
<p>但是！PyTorch早就为我们准备好了一个方便便捷的方法啦！<br>好多盆友使用的<code>torchvision.datasets.ImageFolder</code>，在这里同样附上<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wL19tb2R1bGVzL3RvcmNodmlzaW9uL2RhdGFzZXRzL2ZvbGRlci5odG1sI0ltYWdlRm9sZGVy" title="https://pytorch.org/docs/1.1.0/_modules/torchvision/datasets/folder.html#ImageFolder">源码<i class="fa fa-external-link"></i></span>，这个类就是继承的同文件下的DatasetFolder，只是封装好了一组后缀，也没有提供新的方法，所以这里以DatasetFloder的源码展开看一下。</p>
<p>文件夹格式留意看：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root/class_x/xxx.ext</span><br><span class="line">root/class_x/xxy.ext</span><br><span class="line">root/class_x/xxz.ext</span><br><span class="line"></span><br><span class="line">root/class_y/123.ext</span><br><span class="line">root/class_y/nsdf3.ext</span><br><span class="line">root/class_y/asd932_.ext</span><br></pre></td></tr></table></figure></p>
<p>首先看参数：<code>__init__(self, root, loader, extensions=None, transform=None, target_transform=None, is_valid_file=None)</code>：<br>root是根路径；loader是一个callable的方法，<em>传入一个路径，读出一个东西</em>；ext是指定后缀；后边两个transform；最后一个参数也是一个callsble的方法，判断某个文件是否是符合我们需要的方法。下边看一下这个类的运行流程：</p>
<ol>
<li>读入root下的文件夹<strong>的名字</strong>，将名字对应转换成数字序列（0、1、2、3）这种形式。也就是说，无论label怎么存，都可以！<br>也就是这段代码<code>class_to_idx = {classes[i]: i for i in range(len(classes))}</code></li>
<li>下边进入<code>make_dataset</code>这个方法，这个方法会遍历root下所有的文件，如果指定了<code>is_valid_file</code>则使用这个函数判断文件是否合法，否则依照传入的ext后缀判断这个文件是否是我们需要的。<br>经过一连串的判断，最后返回N个由<strong>地址和label</strong>组成的二元tuple，N是文件个数，label是数字形式的。</li>
<li>在getitem时，会调用loader进行读取数据，读取完后会对数据和label做transform。然后返回。<br>当然在这里我们可以定义自己的loader。如果我们自定义的loader读上来就是pkl，那么就不需要再定义transform了，非常简单。</li>
<li>最后<code>return sample, target</code>，那么在for的每次迭代时就可以拿到这两个元素。</li>
</ol>
<p><strong>三代目</strong>步骤：<br>这个类把我们在<a href="再改进">再改进</a>中的第2步骤都实现了有没有！而且还能配合上DataLoader的shuffle！现在<strong>可能</strong>只留下第1步需要我们做(如果不转pkl，那么什么也不用做，直接使用默认的loader即可)。<br>或者我们提前把测试集和训练集划分开，然后定义两个DatasetFolder就行啦!</p>
<h1 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataset = Datasets.ImageFolder(img_path, transform=transforms.ToTensor())</span><br><span class="line">train_dataset = Datasets.DatasetFolder(img_path, extensions=<span class="string">'pkl'</span>, loader=pklloder)</span><br></pre></td></tr></table></figure>
<p>如果直接使用默认的图片后缀，如jpg、png等，可以直接使用ImageFolder即可，使用transform将读入的图片进行一些操作。因为我的图片都已经裁剪为<code>224*224</code>了，所以只需要totensor就行。如果像我一样使用了pkl，那么就需要自己写一个loader函数。</p>
<p>如果不使用shuffle，那么读入的顺序就像这样：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_035.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_036.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_037.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_038.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_039.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_040.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_041.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_042.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">0044</span>_043.pkl</span><br></pre></td></tr></table></figure><br>如果使用了shuffle，那么就变成了这样：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">./crops/<span class="number">4</span>/<span class="number">0230</span>_509.pkl</span><br><span class="line">./crops/<span class="number">1</span>/<span class="number">1483</span>_064.pkl</span><br><span class="line">./crops/<span class="number">3</span>/<span class="number">1553</span>_005.pkl</span><br><span class="line">./crops/<span class="number">2</span>/<span class="number">1742</span>_005.pkl</span><br><span class="line">./crops/<span class="number">3</span>/<span class="number">1016</span>_002.pkl</span><br><span class="line">./crops/<span class="number">4</span>/<span class="number">0230</span>_564.pkl</span><br><span class="line">./crops/<span class="number">4</span>/<span class="number">2003</span>_036.pkl</span><br><span class="line">./crops/<span class="number">2</span>/<span class="number">2607</span>_008.pkl</span><br><span class="line">./crops/<span class="number">3</span>/<span class="number">0450</span>_004.pkl</span><br></pre></td></tr></table></figure></p>
<p>使用scikit-learn进行分层的划分数据，即按照label的比例划分：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">        X, y, test_size=<span class="number">0.1</span>, stratify=y, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<div class="note info">
            <p>一个<code>224*224*3</code>大小的图片，png要80~90kB，jpg格式的要10~30kB，而pkl的要588KB！</p>
          </div>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p>[1]<span class="exturl" data-url="aHR0cHM6Ly93d3cucHl0b3JjaHR1dG9yaWFsLmNvbS9weXRvcmNoLWN1c3RvbS1kYXRhc2V0LWV4YW1wbGVzLw==" title="https://www.pytorchtutorial.com/pytorch-custom-dataset-examples/">PyTorch 中自定义数据集的读取方法小结<i class="fa fa-external-link"></i></span><br>[2]<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1NTA1NDUvYXJ0aWNsZS9kZXRhaWxzLzkwMzczMjY0" title="https://blog.csdn.net/u011550545/article/details/90373264">pytorch加载大数据<i class="fa fa-external-link"></i></span><br>[3]<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1NTA1NDUvYXJ0aWNsZS9kZXRhaWxzLzg3ODQyNzYw" title="https://blog.csdn.net/u011550545/article/details/87842760">pytorch load huge dataset<i class="fa fa-external-link"></i></span><br>[4]<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzLzEuMS4wLw==" title="https://pytorch.org/docs/1.1.0/">Pytorch 1.1.0 Docs<i class="fa fa-external-link"></i></span></p>
<p>在下才疏学浅，如有描述有误的地方，还望不吝赐教。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>知其所以然</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>CrossEntropyLoss之我见</title>
    <url>/2019/08/CrossEntropyLoss/</url>
    <content><![CDATA[<p>看了一天关于各种熵的解释，比较好的文章已经添加到了<a href="/bookmark/">Bookmark</a>。本文中的公式均来自<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5mdW5jdGlvbmFsLmh0bWw=" title="https://pytorch.org/docs/stable/nn.functional.html">PyTorch文档<i class="fa fa-external-link"></i></span>。</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>把数据缩放到$[0,1]$，且和为$1$。对每个元素进行了操作，结果并不改变维数，即输入N维，输出还是N维。</p>
<script type="math/tex; mode=display">
\operatorname{Softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}</script><h2 id="LogSoftmax"><a href="#LogSoftmax" class="headerlink" title="LogSoftmax"></a>LogSoftmax</h2><p>结果似乎比Softmax更加稳定，结果的范围为$[-inf,0)$。结果维数依然不变。</p>
<script type="math/tex; mode=display">
\log \operatorname{Softmax}\left(x_{i}\right)=\log \left(\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\right)</script><a id="more"></a>
<h2 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h2><p>negative log likelihood loss，负对数似然损失函数，对Log后的结果进行损失计算，例如Logsoftmax。<script type="math/tex">l_{n}=-w_{y_n}x_{n,y_n}</script>，如果不传入<em>weight</em>，那么<em>w</em>就是1，所以<script type="math/tex">l_{n}=-x_{n,y_n}</script>，即<em>input</em>的负数，<code>reduction=mean</code>也就变成了直接除以$N$。</p>
<script type="math/tex; mode=display">
\ell(x, y)=\left\{
      \begin{array}{ll}
      {\{l_{1}, \dots, l_{N}\},} & {\text{if reduction}=\text{'none';}} \\
      {\sum_{n=1}^{N} l_{n},} & {\text{if reduction}=\text {'sum';}} \\
      {\sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{y_n}} l_{n},} & {\text{if reduction}=\text{'mean'.}} \\
      \end{array}\right.</script><h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>关于PyTorch<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1sI2Nyb3NzZW50cm9weWxvc3M=" title="https://pytorch.org/docs/stable/nn.html#crossentropyloss">官网<i class="fa fa-external-link"></i></span>的CrossEntropyLoss补充了几组小实验。<br>文档中给出的CrossEntropyLoss的公式如下(以下讨论的均不带<em>weight</em>)：</p>
<script type="math/tex; mode=display">
\operatorname{loss}(x, class)=-\log \left(\frac{\exp (x[class])}{\sum_{j} \exp (x[j])}\right)</script><p>其中的<script type="math/tex">\exp (x[class])</script>越大，就是说当这个类别对应的值越大，即占的比例越大，<em>loss</em>值越小，结果越靠近0。<br>用一个样本简单模拟一下这个过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">1</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input</span><br><span class="line">tensor([[<span class="number">-1.4976</span>,  <span class="number">0.1278</span>,  <span class="number">1.6863</span>, <span class="number">-0.3295</span>,  <span class="number">1.4173</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">1</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target</span><br><span class="line">tensor([<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss(input, target)</span><br><span class="line">tensor(<span class="number">2.7809</span>, grad_fn=&lt;NllLossBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = input.detach().numpy()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = i[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i</span><br><span class="line">array([<span class="number">-1.4976473</span> ,  <span class="number">0.12782747</span>,  <span class="number">1.6863296</span> , <span class="number">-0.32949358</span>,  <span class="number">1.4173082</span> ],</span><br><span class="line">      dtype=float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.log(np.sum(np.exp(i))/np.exp(i[<span class="number">3</span>]))</span><br><span class="line"><span class="number">2.7809231</span></span><br></pre></td></tr></table></figure>
<p>文档中提到，对于多batch的情况，<em>The losses are averaged across observations for each minibatch.</em></p>
<p>还有一句话，<em>This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.</em>所以上边给出的公式就是负的LogSoftmax，<em>负号把取值区间变成了(0,inf]，越靠近0，则结果越好，这样只需要最小化loss即可</em>，下面看一下CrossEntropyLoss源码，两者是怎么结合的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> nll_loss(log_softmax(input, <span class="number">1</span>), target, weight, <span class="literal">None</span>, ignore_index, <span class="literal">None</span>, reduction)</span><br></pre></td></tr></table></figure>
<p>先对它进行logsoftmax，然后再nllloss，默认的参数<code>reduction=&#39;mean&#39;</code>，所以出现了上边所说的，结果是各个minibatch的均值。下边进行一下多minibatch的实验，分别指定<code>reduction</code>为<code>mean</code>和<code>none</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss(input, target)</span><br><span class="line">tensor([<span class="number">1.2951</span>, <span class="number">1.9074</span>, <span class="number">1.6085</span>], grad_fn=&lt;NllLossBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = input.detach().numpy()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i</span><br><span class="line">array([[ <span class="number">1.7035984</span> , <span class="number">-0.8095179</span> , <span class="number">-0.22653757</span>, <span class="number">-0.19294807</span>,  <span class="number">1.0479178</span> ],</span><br><span class="line">       [ <span class="number">0.37498614</span>, <span class="number">-0.26111123</span>, <span class="number">-2.1978652</span> ,  <span class="number">0.80966765</span>, <span class="number">-0.5037327</span> ],</span><br><span class="line">       [ <span class="number">0.87008125</span>, <span class="number">-1.5282617</span> ,  <span class="number">0.25803488</span>, <span class="number">-1.0827187</span> ,  <span class="number">2.0395918</span> ]],</span><br><span class="line">      dtype=float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i=i[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i</span><br><span class="line">array([ <span class="number">1.7035984</span> , <span class="number">-0.8095179</span> , <span class="number">-0.22653757</span>, <span class="number">-0.19294807</span>,  <span class="number">1.0479178</span> ],</span><br><span class="line">      dtype=float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.log(np.sum(np.exp(i))/np.exp(i[<span class="number">4</span>]))</span><br><span class="line"><span class="number">1.2950675</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = input.detach().numpy()[<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.log(np.sum(np.exp(i))/np.exp(i[<span class="number">1</span>]))</span><br><span class="line"><span class="number">1.9073898</span></span><br></pre></td></tr></table></figure>
<p>可以看到，已经计算出了<code>reduction</code>为<code>none</code>时的值，下边设为<code>mean</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss_mean = nn.CrossEntropyLoss(reduction=<span class="string">'mean'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss_mean(input, target)</span><br><span class="line">tensor(<span class="number">1.6037</span>, grad_fn=&lt;NllLossBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1.2951</span> + <span class="number">1.9074</span> + <span class="number">1.6085</span>)/<span class="number">3</span></span><br><span class="line"><span class="number">1.6036666666666666</span></span><br></pre></td></tr></table></figure>
<h3 id="恍然大悟"><a href="#恍然大悟" class="headerlink" title="恍然大悟"></a>恍然大悟</h3><p>我一直有个问题，就是交叉熵的公式分明是<script type="math/tex">H(p, q)=-\sum_{x} p(x) \log q(x)</script>，但是怎么到了CrossEntropyLoss里边$p(x)$就不见了，<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzU2NzQ1MS9hcnRpY2xlL2RldGFpbHMvODA4OTUzMDk=" title="https://blog.csdn.net/weixin_37567451/article/details/80895309">《经典损失函数：交叉熵》<i class="fa fa-external-link"></i></span>这篇文章写的非常好，不仅解释了为什么交叉熵要和Softmax一起用，而且让我明白了$p(x)$去哪了。</p>
<p>我直接看的是PyTorch的源码，这里给了我一点误解。这篇文章中提到，CrossEntropyLoss实际上应该是先计算Softmax，把分布转换到概率分布，得到$q(x)$，然后在计算交叉熵。在计算交叉熵的时候，$p(x)$是<strong>one-hot</strong>之后的编码，也就是说只有对应的类别的那个值是1，其他的都是0，也就得到了公式<script type="math/tex">H(p, q)=- \log q(x_{class})</script>，消去了求和记号和$p(x)$，再把<script type="math/tex">q(x_{class})</script>替换成对应的Softmax公式，即可得到了PyTorch中给出的CrossEntropyLoss公式。但是PyTorch中不是这么组织这个过程的，它是使用了$LogSoftmax+NLLLoss$的形式，其实实现的相同的方法。<br>再次需要注意的是，NLLLoss中的<script type="math/tex">1/N</script>和<script type="math/tex">\sum</script>，是计算的不同样本之间结果，和交叉熵中的求和记号区分开。</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>loss function</tag>
        <tag>知其所以然</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Next 7.X 添加背景</title>
    <url>/2019/07/Next-7-X-Background/</url>
    <content><![CDATA[<p>Next 7.X (主要是7.2.0)添加背景跟以前的版本有了很大的变化，网上很多方法都是旧版Next的，下边是我自己尝试出来的方法：<br>路径：<code>themes\next\source\css\_common\scaffolding\base.styl</code></p>
<a id="more"></a>
<p>修改其中的body，把background字段覆盖：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// background: $body-bg-color;</span><br><span class="line">background: url(/images/XXX.jpg);</span><br><span class="line">background-size: cover;</span><br><span class="line">background-repeat: no-repeat;</span><br><span class="line">background-attachment: fixed;</span><br><span class="line">background-position: center;</span><br></pre></td></tr></table></figure><br>然后如果想让前边的页面虚化一些可以在最后直接添加：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.main-inner &#123;</span><br><span class="line">    opacity: 0.9;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>最后剩下的一部分就不要虚化了，因为会导致搜索框太浅，无法使用。</p>
<p>如果不添加背景，可以直接给背景改个颜色，我这里最后还是采取了直接改颜色的办法，可以看到背景颜色是由<code>$body-bg-color</code>决定的，可以在当前主题中搜索这个关键词，例如我这个主题是Pisces，所以我的路径是在<code>themes\next\source\css\_variables\Pisces.styl</code>里，直接把这一项的颜色改一下就行，我改成了<code>#f2f3f4</code>，就是灰的颜色重了一点，可能也看不出啥变化~</p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo 3.9</tag>
        <tag>NexT 7</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo with VSCode</title>
    <url>/2019/07/Hexo-with-VSCode/</url>
    <content><![CDATA[<div class="note warning">
            <p>惨遭翻车，7.4.0更新<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy9uZXh0LTctNC0wLXJlbGVhc2VkLw==" title="https://theme-next.org/next-7-4-0-released/">link<i class="fa fa-external-link"></i></span>：<code>fi</code>成为过去了，取消了一个这么好用的功能。以下功能关于<code>fi</code>的部分仅适用于7.4.0之前的版本，之后会再更新。</p>
          </div>
<a id="more"></a>
<p>介绍一下怎么在vscode中预览hexo的图片~</p>
<div class="note warning">
            <p>本文提到的方法有两个前提：<br>1.适用于把所有素材放到images下时；<br>2.适用于使用NexT主题时</p><p>开启了<code>post_asset_folder</code>功能的请参考reference的链接和本文修改。使用其他主题可以参考本文修改。</p>
          </div>
<h1 id="Paste-Image"><a href="#Paste-Image" class="headerlink" title="Paste Image"></a>Paste Image</h1><p>首先需要安装插件Paste Image，这个插件是用来粘贴图片的，直接把图片复制，然后在vscode中粘贴即可，十分方便。</p>
<blockquote>
<p>需要注意的是，当粘贴的时候，只需要将文件名用鼠标圈住，然后再粘贴，就可以把圈住的文件名当作粘贴后的文件名，十分方便。具体操作可见插件说明页。</p>
</blockquote>
<p>安装好后在当前项目的<code>.vscode/setting.json</code>中写上以下配置项，这些配置项只会影响当前项目，不推荐写入系统配置项。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"pasteImage.namePrefix"</span>: <span class="string">"$&#123;currentFileNameWithoutExt&#125;-"</span>,</span><br><span class="line">    <span class="attr">"pasteImage.path"</span>: <span class="string">"$&#123;projectRoot&#125;/source/images"</span>,</span><br><span class="line">    <span class="attr">"pasteImage.basePath"</span>: <span class="string">"$&#123;projectRoot&#125;/source"</span>,</span><br><span class="line">    <span class="attr">"pasteImage.forceUnixStyleSeparator"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"pasteImage.prefix"</span>: <span class="string">"/"</span>,</span><br><span class="line">    <span class="attr">"pasteImage.insertPattern"</span>: <span class="string">"&#123;% fi $&#123;imageFilePath&#125;,,, %&#125;"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解释一下，意思是使用(当前文件名-圈住的名字)当前缀，然后粘贴到/images/XXX下。上个图一看：</p>
<div class="note info">
            <p>为什么要加4个逗号，因为使用的是NexT的图片解析方式，这种解析方式只有最后一个字段是控制图片宽度的。可以搜索前一篇关于Hexo的文章看一下两者的区别。如果使用其他theme的同学可以只把<code>&quot;pasteImage.insertPattern&quot;</code>这一项改一下，改成对应的图片解析格式即可。</p>
          </div>
<h1 id="Markdown-Preview-Enhanced"><a href="#Markdown-Preview-Enhanced" class="headerlink" title="Markdown Preview Enhanced"></a>Markdown Preview Enhanced</h1><p>这个插件是用来预览的，支持自定义扩展，安装好后<code>ctrl+shift+P</code>输入<code>Markdown Preview Enhanced: Extend Parser</code>，修改对应部分：<br><figure class="highlight js"><table><tr><td class="code"><pre><span class="line">onWillParseMarkdown: <span class="function"><span class="keyword">function</span>(<span class="params">markdown</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>)=&gt;</span> &#123;</span><br><span class="line">      markdown = markdown.replace(</span><br><span class="line">        /\&#123;%\s*fi\s*([\w/\.-]*)\s*,.*,.*,.*%\&#125;/g,</span><br><span class="line">        (whole, content) =&gt; (<span class="string">`&lt;center&gt;&lt;img src="../<span class="subst">$&#123;content&#125;</span>" width="300" /&gt;&lt;/center&gt;`</span>)</span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">return</span> resolve(markdown)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;,</span><br></pre></td></tr></table></figure><br>可以看到的是，它只是使用正则，把刚才粘贴进来的格式，解析成了html显示图片的格式，宽度使用固定值300，这样即可在预览里看到刚才粘贴的图片。<strong>文件名支持<code>.</code>，<code>-</code>，数字，英文字符</strong>。<br><strong>这个预览的时候只是用来看一下粘贴好了没，而真正发布的时候使用的参数是md文件里写到的参数。</strong></p>
<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>闲暇的时候现学了俩小时js，然后改了改。为什么只做了下边这几个，因为这几个参数容易控制，例如<code>fi</code>更容易把宽度解析出来。使用<code>img</code>的同学可以参考未改动之前的进行修改。<br><figure class="highlight js"><table><tr><td class="code"><pre><span class="line">onWillParseMarkdown: <span class="function"><span class="keyword">function</span>(<span class="params">markdown</span>) </span>&#123;</span><br><span class="line">    onWillParseMarkdown: <span class="function"><span class="keyword">function</span>(<span class="params">markdown</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>)=&gt;</span> &#123;</span><br><span class="line">      markdown = markdown.replace(</span><br><span class="line">        /\&#123;%\s*fi\s*([\w/\.-]*)\s*,.*,.*,\s*(\d*)\w*\s*%\&#125;/g,</span><br><span class="line">        <span class="string">`&lt;center&gt;&lt;img src="../$1" width="$2" /&gt;&lt;/center&gt;`</span></span><br><span class="line">      ).replace(</span><br><span class="line">        /\&#123;%\s*label\s*(|<span class="keyword">default</span>|primary|success|info|warning|danger)\s*\@\s*([\u4e00-\u9fa5]+)\s*%\&#125;/g,</span><br><span class="line">        <span class="string">`==$2==`</span></span><br><span class="line">      ).replace(</span><br><span class="line">        /\&#123;%\s*note.*%\&#125;\s*\n*(.*)\n*\&#123;%\s*endnote\s*%&#125;/g,</span><br><span class="line">        <span class="string">`&gt; $1`</span></span><br><span class="line">      )</span><br><span class="line">      <span class="keyword">return</span> resolve(markdown)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;,</span><br></pre></td></tr></table></figure><br>上边使用了一个链式得替换，逐步把markdown文件中的所需部分进行替换。一共替换了3次，则实现了3个功能：</p>
<ul>
<li><code>fi</code>的解析，同时也把写好的宽度解析出来了，例如宽度是<code>200px</code>，<code>200</code>都可以</li>
<li><code>label</code>的解析，使用这个插件进行预览的时候可以进行高亮设置，于是无论用的那个参数都进行了高亮</li>
<li><code>note</code>缺点是note只支持一行note的解析，匹配多行调不好，但是平常用是没问题了<br>其他的问题就诸如<code>{\% \%}</code>必须要连在一起写啊这都是习惯问题了，我平常习惯这么写了，这个就没改。</li>
</ul>
<p>在这里再推荐一个<em>Markdown All in One</em>，功能也很强大，比如直接插入链接等，很实用。</p>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p>《利用vscode插件与git hook提升hexo编写部署体验》<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9hMTE3NjUwZjZjNzY=" title="https://www.jianshu.com/p/a117650f6c76">link<i class="fa fa-external-link"></i></span><br>菜鸟工具，在线匹配正则，如果要修改正则的话可以用这个看看 <span class="exturl" data-url="aHR0cHM6Ly9jLnJ1bm9vYi5jb20vZnJvbnQtZW5kLzg1NA==" title="https://c.runoob.com/front-end/854">link<i class="fa fa-external-link"></i></span><br>w3school JavaScript replace() 方法 <span class="exturl" data-url="aHR0cDovL3d3dy53M3NjaG9vbC5jb20uY24vanNyZWYvanNyZWZfcmVwbGFjZS5hc3A=" title="http://www.w3school.com.cn/jsref/jsref_replace.asp">link<i class="fa fa-external-link"></i></span> <code>$n</code>的写法就是从这里学来的<br><span class="exturl" data-url="aHR0cHM6Ly9zaGQxMDF3eXkuZ2l0aHViLmlvL21hcmtkb3duLXByZXZpZXctZW5oYW5jZWQvIy9leHRlbmQtcGFyc2Vy" title="https://shd101wyy.github.io/markdown-preview-enhanced/#/extend-parser">官方文档<i class="fa fa-external-link"></i></span></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo 3.9</tag>
        <tag>NexT 7</tag>
      </tags>
  </entry>
  <entry>
    <title>Build a Hexo Blog</title>
    <url>/2019/07/Build-a-Hexo-Blog/</url>
    <content><![CDATA[<p>我这个博客搭建的全部工作。更加详细的内容可以参考<a href="/categories/Hexo/">categories</a>，后边有些新的文章。</p>
<a id="more"></a>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h2><p>使用的是Nodejs 10，从官网下载的时候发现Nodejs 10的支持时间还很长，所以就用的这个，第一个接触这个东西，不是很懂。</p>
<p>受制于用啥软件都要用最新版的强迫症思想，去查了查怎么升级Nodejs，然后<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1emhhbzU5My9hcnRpY2xlL2RldGFpbHMvODE3MTIwMTY=" title="https://blog.csdn.net/guzhao593/article/details/81712016">node版本如何升级<i class="fa fa-external-link"></i></span>中提到了一句话：</p>
<!-- >爬坑后的结论：window系统升级node只能到node官网下载window安装包来覆盖之前的node。 -->
<div class="note warning">
            <p>爬坑后的结论：window系统升级node只能到node官网下载window安装包来覆盖之前的node。</p>
          </div>
<p>所以安安静静的用吧，先别想着升级了，<code>node -v</code>查看版本，我的版本是<em>v10.15.3</em>。</p>
<h2 id="npm"><a href="#npm" class="headerlink" title="npm"></a>npm</h2><p>下载了nodejs之后就可以用npm管理包了，同样是第一次用。类似Python的pip，安装包的。但是和pip不太一样的是，可以选择把包装在项目文件里还是装在全局，所以在安装时就会有点迷糊。</p>
<p><code>npm install xxx -g</code> 安装在全局，可能就安装到了node的安装目录，和pip一样，安装好的项目可以全局使用，比如直接在命令行里用。如果不加<code>-g</code>参数就是安装到了当前项目，即在当前项目下新建一个<code>node_modules</code>文件夹，然后把包装进去，可以在项目代码里引入。所以安装的时候一定要注意安装位置。还可以指定<code>-save</code>，意思是将模块安装到项目目录下，并在package文件的dependencies节点写入依赖。关于他们的具体区别可以看这篇<span class="exturl" data-url="aHR0cHM6Ly93d3cubGltaXRjb2RlLmNvbS9kZXRhaWwvNTlhMTViMWE2OWU5NTcwMmUwNzgwMjQ5Lmh0bWw=" title="https://www.limitcode.com/detail/59a15b1a69e95702e0780249.html">文章<i class="fa fa-external-link"></i></span>，就不多说了。<br>有了这些知识，去看Hexo的官网的相关信息就不会迷糊了。</p>
<p>使用<code>npm install npm -g</code>命令可以方便的升级npm，<em>这样我心里舒服多了</em>。<br><code>npm list -g --depth 0</code>查看全局安装包，<code>npm outdated -g --depth=0</code>查看全局能升级的。同理，不加<code>-g</code>就是看当前项目的咯。<code>npm list xxx</code>查看某个包。</p>
<p>其他的用法还有uninstall/ls/update/search，详见<span class="exturl" data-url="aHR0cHM6Ly93d3cucnVub29iLmNvbS9ub2RlanMvbm9kZWpzLW5wbS5odG1s" title="https://www.runoob.com/nodejs/nodejs-npm.html">runoob<i class="fa fa-external-link"></i></span></p>
<h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>安装过程详见官网，有了上边的准备知识就不难了。<br>安装完成后有如下几个部分：</p>
<ul>
<li><strong>_config.yml</strong> 网站配置信息，这里边可以设置的不多，theme的设置项非常多</li>
<li><strong>package.json</strong> 包信息，使用的包用了<code>-save</code>在这里都会有记载，所以安装包的时候只需要把使用<code>-save</code>，那么把当前项目<span class="label info">写作分支</span>同步到git后，更换设备直接安装这里边的项目即可</li>
<li><strong>scaffolds</strong> 模板文件，其实就是md文件头模板吧~</li>
<li><strong>source</strong> 新建的文章都在这里，还有其他资源，比如images</li>
<li><strong>themes</strong> 下载的主题</li>
<li><strong>node_modules</strong> 安装在当前项目的包都在这里，这个在gitignore里不会同步这个文件，因为换了设备的时候使用npm重新安装即可</li>
<li><strong>.gitignore</strong> 这个应该是下载hexo的时候hexo的git信息，<em>有个问题是怎么利用这个给项目备份，换电脑了还能把这些文件恢复回来，是个问题，还在研究</em></li>
</ul>
<p>从官网的配置页面很容易就能修改_config.yml文件。</p>
<h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><p><code>hexo new [layout] &lt;title&gt;</code> 新建一个文章~标题文章过长记得加引号，支持中文。</p>
<p><code>hexo generate/g</code> 生成静态文件，可以直接部署<code>-d/--deploy</code>或者监视<code>-w/--watch</code>，监视的意思就是边改边生成静态文件~<br>也可以用<code>hexo server/s</code>的形式，边改边看。</p>
<p><code>hexo deploy/d</code> 部署，<code>-g/--generate</code>同时生成静态文件。<br><strong><code>g -d</code>和<code>d -g</code>是一样的</strong></p>
<p><code>hexo clean</code> 清除缓存，清除缓存文件<code>db.json</code>和已生成的静态文件<code>public</code></p>
<p><code>hexo list &lt;type&gt;</code> 可以查看网站的各种信息，可用的参数有<code>page, post, route, tag, category</code>，很好~</p>
<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><h3 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h3><p>有三种默认布局，post会发一个文章到博客的Archives类别里；page会新建一个标签页，和Archives是一个级别的，<em>需要去主题页面加入生成的页面，才能正常解析</em>，例如可以添加About me页面；draft是草稿。<br>可以定义很多布局，比如一个categories就有一个布局，这样每次新写文章的时候直接new对应的布局就行了。据我尝试，新建的布局都会被放到<code>_post</code>目录下当作文章看待。</p>
<p>布局中的title对应着正文中文章名称，而md文件的名称则对应着url解析时对应的文章名称。</p>
<h3 id="草稿"><a href="#草稿" class="headerlink" title="草稿"></a>草稿</h3><p>这个模式还挺有用的，<code>hexo publish [layout] &lt;title&gt;</code>可以发布写好的草稿，有些文章舍不得删除也可以直接拖进<code>_drafts</code>文件夹，就成了草稿，再拖回<code>_posts</code>就有成了可以发布的文章。<br><code>hexo server --drafts</code>可以预览草稿</p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>可以使用两种分类方式，一个是<code>categories</code>，另外一个是<code>tags</code>，使用方式是有两种：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tags/categories:</span><br><span class="line">- t1</span><br><span class="line">- t2</span><br><span class="line"></span><br><span class="line">tags/categories: [t1, t2]</span><br></pre></td></tr></table></figure><br>区别是categories包含层级关系，tags则全是并列关系~</p>
<h3 id="正文中的标签插件"><a href="#正文中的标签插件" class="headerlink" title="正文中的标签插件"></a>正文中的标签插件</h3><p>这一类是hexo解析的特殊格式，可以快速在正文中插入所需的东西。算是给markdown做了个扩展吧，感觉这部分比较好。<br><strong>问题是很多编辑器没有办法预览这个部分</strong>，所以最好以server的形式看，也可以修改vscode中的配置信息使其兼容图片的预览。我更喜欢用原生markdown，除了图片可以用这个外，其他的部分如果使用原生markdown可以直接用插件预览。（vscode预览详见下一篇文章）</p>
<h4 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% blockquote [author[, source]] [link] [source_link_title] %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure>
<p><em>别名quote</em>，代替了<code>&gt;</code>的引用形式，增加了一些解释字段，内容更加丰富。</p>
<blockquote><p>One meets its destiny on the road he takes to avoid it.</p>
<footer><strong>Kung Fu Panda</strong><cite><span class="exturl" data-url="aHR0cHM6Ly9tb3ZpZS5kb3ViYW4uY29tL3N1YmplY3QvMTc4MzQ1Ny8=" title="https://movie.douban.com/subject/1783457/">douban<i class="fa fa-external-link"></i></span></cite></footer></blockquote>
<h4 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% codeblock [title] [lang:language] [url] [link text] %&#125;</span><br><span class="line">code snippet</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure>
<p><em>别名code</em>，相比原生代码块，增加了对引用网址等信息的解析。</p>
<h4 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% img [class names] /path/to/image [width] [height] [title text [alt text]] %&#125;</span><br></pre></td></tr></table></figure>
<p>这个class names还是<strong>没搞懂</strong>啊，应该是对应的css文件，直接修改图片属性。<em>路径是source目录下的图片。</em><br>其实我还是更喜欢使用<code>source/images</code>的形式传入图片，然后<code>![](/images/image.jpg)</code>引入图片，问题是预览的时候根路径是当前路径，难以预览。</p>
<p>可以使用<code>post_asset_folder: true</code>设置为一个文章对应一个文件夹，这样会导致文件夹太多了，作为一个强迫症看着心里难受，还是扔一个文件夹好。在这个模式下，官方推荐使用{<code>% asset_img name image %</code>}方式引入图片，不会在预览的时候消失。这个命令会自动在同名文件夹下读取这个文件夹的资源了。我没这么做哦。</p>
<img class width="200" height="200" title="Linux" data-src="/images/Build-a-Hexo-Blog-linux.jpg">
<h4 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% link text url [external] [title] %&#125;</span><br></pre></td></tr></table></figure>
<p>它和默认的Link相比就是链接text之间允许空格</p>
<p>除了上述命令外，还有：加入一个iframe，就是嵌入一个子网页；source中的代码文件；Youtube视频；引用文章或文章中的资源等。</p>
<h2 id="推送到Github"><a href="#推送到Github" class="headerlink" title="推送到Github"></a>推送到Github</h2><p><code>hexo clean &amp;&amp; hexo deploy</code>，这个好像不用<code>-g</code>参数也会重新生成。<br><em>记得推送到master分支啊，尝试推送到别的分支github.io并不会更新</em>，这个推送了<span class="label info">部署分支</span></p>
<div class="note info">
            <h3 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h3><p>个人感觉可以直接使用下载时的gitignore，然后把项目推送到写作分支，然后再重新部署的只需要<code>git clone</code>下来项目文件，然后用<code>npm install</code>即可，因为在使用hexo建站的时候就是类似的流程，而所需的model在安装时都被记录下来了，详见新更新的文章<a href="/2019/11/update-to-4-0/">link</a>。</p>
          </div>
<h1 id="NexT"><a href="#NexT" class="headerlink" title="NexT"></a>NexT</h1><div class="note info">
            <p>本文使用的均是NexT 7.X，新版本的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0" title="https://github.com/theme-next/hexo-theme-next">Github<i class="fa fa-external-link"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy8=" title="https://theme-next.org/">官网<i class="fa fa-external-link"></i></span>。 </p>
          </div>
<p><strong>使用了NexT后，会重新解析所有布局，包括图片和<code>quote</code>。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<p>目录下直接执行这个下载最新的，好处是以后可以直接用git更新~ <code>git pull</code>就可以更新这个主题，但是可能会遇到冲突，需要懂一点git知识，才能合并。（这个是我猜的，没试过…）<br>参考新更新的文章<a href="/2019/11/update-to-4-0/">link</a>，如何把配置提取出来，就不需要处理冲突了。</p>
<h2 id="Theme-Setting"><a href="#Theme-Setting" class="headerlink" title="Theme Setting"></a>Theme Setting</h2><p>NexT的官网的Theme setting页面做得很好，跟配置页面几乎同步在介绍各种设置的作用。下边这些都是在主题的配置页中：</p>
<ul>
<li>修改<code>favicon</code>可以修改浏览器标题前的那个小图标~</li>
<li>修改<code>avatar</code>可以放上个人的照片</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5neXUxMDEzL2FydGljbGUvZGV0YWlscy84MDY0OTc3NA==" title="https://blog.csdn.net/jiangyu1013/article/details/80649774">关于许可协议的介绍<i class="fa fa-external-link"></i></span>可以看看这个，添加许可协议</li>
<li><code>mobile_layout_economy: true</code> 可以设置自适应窄屏</li>
<li>logo就是在上边添加一个logo，不能修改其他的，可能没什么用</li>
<li>各种小图标来自<span class="exturl" data-url="aHR0cHM6Ly9mb250YXdlc29tZS5jb20vdjQuNy4wL2ljb25zLw==" title="https://fontawesome.com/v4.7.0/icons/">Font Awesom v4.7<i class="fa fa-external-link"></i></span>，可以去这里看，然后把icon的名字记下来就行了，比如修改footer图标，我用的这个是bug</li>
<li>有三种方式控制首页文章的长度，可以混合用，效果更好哦<ul>
<li>第一种是可以设置description字段在每篇文章的开头，这个在预览的时候只会出现这句话，同时这句话还会出现在文章标题的下边，以小字的形式出现</li>
<li>第二种是在文章任意部分添加<code>&lt;!-- more --&gt;</code>，就会从这里截断，<strong>这是Hexo推荐的</strong>，比如有些博主喜欢每个文章的预览都是一个图片，就可以用这个，开头放上图片，然后截断。（我现在也是用的这个。新版本的NexT支持在home页的预览中也保持文章的结构，而不像旧版，预览总是纯文字，代码和公式都表现不出来，现在这样很好看）</li>
<li>第三种是在设置里设置<code>auto_excerpt</code>，控制字数进行截断，Hexo不推荐</li>
</ul>
</li>
<li>代码风格改成了night，觉得这个比较好看，其他的没有改，别的还是默认好看</li>
<li><code>busuanzi_count.enable</code>显示访客量，这是框架自带的，不用接入什么api</li>
<li>搜索功能棒，很简单就可以添加本地搜索</li>
<li><code>back2top</code>全设为true，把返回顶部的按钮从右下角改到sidebar的底端，同时显示阅读进度~</li>
</ul>
<h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><p>（NexT 7.2，新版本的有细微的变化）数学公式的支持，按照官网的要求，先卸载旧的<code>marked</code>，然后安装新的渲染引擎，我用的是<code>hexo-renderer-kramed</code>。然后只需设置<code>math.enable=true</code>就行，因为默认已经写了cnd加速。默认是需要在文章的开头设置<code>mathjax: true</code>才会对当前文章进行math渲染，也可以设置为对每一个文章都渲染，显然没啥必要。</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
a &= b + c \\
  &= d + e + f + g \\
  &= h + i
\end{aligned}
\end{equation}</script><h2 id="标签插件"><a href="#标签插件" class="headerlink" title="标签插件"></a>标签插件</h2><p>NexT也提供了一套标签插件，可以看看和Hexo的哪个好用啊~<em>这个图片突然就显示title了，因为引入了Fancybox。</em></p>
<p>7.4.0更新<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy9uZXh0LTctNC0wLXJlbGVhc2VkLw==" title="https://theme-next.org/next-7-4-0-released/">link<i class="fa fa-external-link"></i></span>：<code>fi</code>成为过去了，取消了一个这么好用的功能。</p>
<p>note的表达更加多样性，还可以在note中使用<code>###</code>写入标题，<em>最多支持三级标题，使用四级标题的时候TOC解析会出问题</em>。对note进行如下设置：<code>style: modern</code>使用新式css，<code>icons: true</code>使用提示的图标。<br>label就是给这段文字加个<span class="label success">颜色</span>。class很通用，都是这几个类别。<br>使用了NexT后，外链Link都会在后边加一个小图标，很nice。<br>Tab和Button和PDF等不常用，没写~</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% note [class] [no-icon] %&#125;</span><br><span class="line">Any content (support inline tags too.io).</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">[class]   : default | primary | success | info | warning | danger.</span><br><span class="line">[no-icon] : Disable icon in note.</span><br><span class="line"></span><br><span class="line">&#123;% label [class]@Text %&#125;</span><br><span class="line">[class] : default | primary | success | info | warning | danger.</span><br><span class="line"></span><br><span class="line">&#123;% video url %&#125;</span><br></pre></td></tr></table></figure>
<h2 id="External-Libraries"><a href="#External-Libraries" class="headerlink" title="External Libraries"></a>External Libraries</h2><p>全部使用cdn加速<br>Fancybox: 对图片进行jq优化，支持大图，可以点开图片后左右切换图片 ✔<br>bookmark: 支持记录当前阅读位置，在浏览器左上角显示一个小书签，下次阅读的时候可以继续看<br>reading_progress: 在浏览器顶端显示阅读位置<br>Progress bar: 载入时在顶部显示进度条 ✔ (这个pace的theme好像只能用一个，这里是<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuaHVic3BvdC5jb20vcGFjZS9kb2NzL3dlbGNvbWUv" title="https://github.hubspot.com/pace/docs/welcome/">官网<i class="fa fa-external-link"></i></span>)<br><del>FastClick: 优化点击动作（7.3开始删除了这个）</del><br>Jquery Lazyload: 图片懒加载 ✔<br>Canvas Nest: 背景jq，点点点点点的这个背景 ✔<br>Canvas Ribbon: 炫酷的条带背景，每次点击鼠标就会换</p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo 3.9</tag>
        <tag>NexT 7</tag>
      </tags>
  </entry>
</search>
